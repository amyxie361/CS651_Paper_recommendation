\section{Experimental Evaluation}
%
Although the focus of this project is to learn a new graph processing framework, we still describe a little bit how we are going to evaluate our proposed solution.
%
\subsection{Dataset} \label{dataset}
%
We are planning to use the DBLP Computer Science Bibliography dataset, which can be downloaded \href{https://dblp.uni-trier.de/xml/}{\underline{here}}.
%
The raw dataset is in XML format, and we will convert it into a graph structure.
%
Since our project includes a keyword filtering feature, we are also planning to crawl the paper contents for text processing and analysis.
%
For each paper in the DBLP data, a link to the entry page of the paper has been provided.
%
We can crawl the contents of the entry page and obtain the abstract as an offline dataset for text processing.
%
For the feature that is based on the user reading histories, we plan to use (1) our own reading histories, and (2) synthetic histories.
%
\subsection{Evaluation Methodology}
%
The evaluation methodologies for both keyword-filtering and reading-hisotry recommendation features are similar.
%
Since evaluating the effectiveness of the recommendations by automatic testing is challenging, one way to evaluate our results is manual evaluation. In this work, we face the classic cold start problem in recommendation system: there is no good data for interested papers according to keywords or reading history. We will use both manually evaluation and citation rank evaluation for results measuring.
%
We are going to perform our proposed algorithms on the DBLP dataset described in Section \ref{dataset}.
%
For keyword-filtering recommendation, a keyword from a list of randomly generated keywords is going to be an input.
%
For reading-history recommendation, a reading list is going to be an input.
%
Then, we manually evaluate how relevant the results are to the input keyword or reading history, respectively.
%
The other way to evaluate our results is based on the number of citations of a paper. The recommendations are effective if they are among the most-cited papers.
%
Given a recommendation ranking list $r_1, r_2, ..., r_m$, we crawl their citations $c_1, c_2, ..., c_m$. Then we re-rank the papers according to the citation, from the most to the least. The ranked papers will be $r_{i_1}, r_{i_2},..., r_{i_m}$. Then we count the number of inversion in the new index ranking. We also measure the rank distance: $D(i_1, ..., i_m) = \frac{1}{m} \sum_{j=1}^m |i_j - j|$.
%


\subsection{Experimental Results}
%
Because of the cold start problem, we display out results through examples. For the keywords recommendation, we select the top 8 papers to be our recommendation. For the reading history recommendation, we randomly select paper in related field and feed these index to be the input. We select the top 5 papers from pattern to be our recommendation.

\subsubsection{Keywords recommendation results}

\begin{table}
	\centering
	\begin{tabular}{lccp{6.5cm}c}
		\toprule
		\textbf{Keywords}	& \textbf{Inverse count} 	& \textbf{rank score} 	&\textbf{Top one title} &\textbf{Top one citation}\\ \midrule
		Machine Learning	& 8					& 1.75			&Support-Vector Networks &33820\\
		Cryptography Security& 5					& 1.125			&State of the Art in Ultra-Low Power Public Key Cryptography for Wireless Sensor Networks &268 \\
		Distributed System	& 8					& 1.75			&Distributed Systems: Principles and Paradigms & 4038\\
		Deep Learning		& 11					& 2.25			&Deep learning via semi-supervised embedding &610\\
		Computer Vision 	& 8					& 1.5				&OpenVIDIA: parallel GPU computer vision & 281\\
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Results for selected keywords recommendation. }
	\label{res:keywordall}
\end{table}


\begin{table}
	\centering
	\begin{tabular}{lp{12cm}c}
		\textbf{Keywords}	& Machine Learning \\ \hline
		\toprule
		\textbf{Rank}		& \textbf{Paper Title} 		& \textbf{Citation} 	\\ \midrule
		1				&Support-Vector Networks 	&33820\\
		2				&Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond &10802\\
		3				&Machine learning in automated text categorization &9095\\
		4				&Application of argument based machine learning to law &3\\
		5				&Data Mining: Practical Machine Learning Tools and Techniques&35014\\
		6				&Very Simple Classification Rules Perform Well on Most Commonly Used Datasets&2060\\
		7				&Investigating statistical machine learning as a tool for software development &59\\
		8				&Machine Learning for User Modeling & 474\\
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{An example for keywords recommendation. }
	\label{res:keywordexp}
\end{table}

\subsubsection{Reading History recommendation results}

\begin{table}
	\centering
	\begin{tabular}{lccc}
		\toprule
		\textbf{Exp ID} 	& \textbf{Inverse count} 	& \textbf{rank score} &\textbf{Top one citation}\\ \midrule
		1	& 2	& 0.8		&75881\\
		2	& 4	& 1.6		& 96 \\
		3	& 1	& 0.4		&20024 \\
		4	& 3	& 1.6		&2796\\
		5	& 3	& 1.2		&10141\\
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Results for reading history recommendation. }
	\label{res:patternall}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{p{6cm}p{6cm}c}
		\toprule
		\textbf{Input paper}		& \textbf{Recommend Paper Title} 		& \textbf{Recommend Paper Citation} 	\\ \midrule
		State of the Art in Ultra-Low Power Public Key Cryptography for Wireless Sensor Networks &A method for obtaining digital signatures and public-key cryptosystems	&20024\\
		Cryptography and Network Security: Principles and Practice				&Handbook of Applied Cryptography &18008\\
		Computational soundness for standard assumptions of formal cryptography				&Securing ad hoc networks &3625\\
		Minimalist cryptography for low-cost RFID tags (extended abstract)				&The Resurrecting Duckling: Security Issues for Ad-hoc Wireless Networks &1639\\
		Securing Mobile Ad Hoc Networks with Certificateless Public Keys				&Mitigating routing misbehavior in mobile ad hoc networks&4594\\
		BeeHiveGuard: a step towards secure nature inspired routing algorithms\\
		A proposed curriculum of cryptography courses\\
		Integration of Quantum Cryptography in 802.11 Networks\\
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{An example for reading history recommendation. }
	\label{res:patternexp}
\end{table}

\subsection{Experimental Analysis}

\begin{table}
	\centering
	\begin{tabular}{llll}
		\toprule
		\textbf{Token}		& \textbf{\# Appearance as keywords} \textbf{Token}		& \textbf{\# Appearance as keywords} \\ \midrule
		web	&23293 &service&17037\\
		image &23082&d&15955\\
		network &22428&algorithm&15906\\
		data &22134&video&15294\\
		software &21866 &services&15019\\
		learning &21299&search&14931\\
		mobile &18176&you&14912\\
		fuzzy &17559&students&14236\\
		control &17389&sensor&13855\\
		security & 17060&graph&13686\\
		\bottomrule
	\end{tabular}
	\vspace{3mm}
	\caption{Top 20 keywords appears in papers. }
	\label{res:tfidf}
\end{table}

